{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and generate a Random Forest model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Meylin Herrera \\\\\n",
    "mhscience525@gmail.com\n",
    "MSc_thesis_landslide_Detection-2019 (Deltares-TUDelft)    \n",
    "Description: to train a Random Forest model for landslides detection. The input data are tables derived from segmentation of optical satellite images (Sentinel-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Required directories\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add your path for:\n",
    "segmentation_tables_path = '' \n",
    "confusion_matrix_output_path = '' \n",
    "model_output_path = '' \n",
    "correlation_graph_output_path = '' \n",
    "feature_importance_output_path = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Read segmentation tables and create data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = segmentation_tables_path\n",
    "\n",
    "# datasets\n",
    "landslides_seg = {}\n",
    "files = glob.glob('data*.csv')\n",
    "for filename in os.listdir(path):  \n",
    "     if filename.endswith('.csv'):\n",
    "        frame = pd.read_csv(path+filename,index_col=False)\n",
    "        frame.rename( columns={'Unnamed: 0':'segment'}, inplace=True)  #rename columns   \n",
    "        landslides_seg[filename[0:12]] = frame   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "landslides_seg.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# landslides_seg['landslide_0_'].head() #check data structure\n",
    "#check landslides = 1/non_landslides segments =0\n",
    "# landslides_seg['landslide_0_'].loc[landslides_seg['landslide_0_']['class']==1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data cleaning and  landslides features computation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminate outliers using z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in landslides_seg:\n",
    "    cols = landslides_seg[key][['ndvi','slope_mean','brightness','ndvi_change','ratio_rg_change','ndvi_change']]\n",
    "    z = np.abs(stats.zscore(cols))\n",
    "    print (\"Maximum Z:\", z.max(), key ) # show that outlier have been detected\n",
    "    landslides_seg[key] = landslides_seg[key][(z <5).all(axis=1)]# observation outside 5 standard deviations is considered as an outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate contextual features: landslide diagnostic features relative to the information contained in the image. It is calculated as the difference between the segment (feature value) and the weighted mean of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbours_relationship(df_train,feature,area):\n",
    "    for key in df_train:\n",
    "        #calculate the weighted mean per feature\n",
    "        weighted_mean = (df_train[key][feature] * df_train[key][area]).sum() /(df_train[key][area].sum())\n",
    "        mean_all_segments = df_train[key][feature].mean()\n",
    "        feature_subtraction_weighted = []        \n",
    "\n",
    "        for i in range (len(df_train[key])):\n",
    "            # Subtract the mean from each observation and squared it\n",
    "              mean_weighted_subtraction = (df_train[key][feature].iloc[i] - weighted_mean) \n",
    "              mean_subtraction  =  (df_train[key][feature].iloc[i] - mean_all_segments) \n",
    "              new_name_feature = feature[0:]+'_var'\n",
    "              new_name_feature_weighted = feature[0:]+'_deviation'    \n",
    "              feature_subtraction_weighted.append (mean_weighted_subtraction)\n",
    "\n",
    "        # Create a new column with the calculated contextual feature\n",
    "        df_train[key][new_name_feature_weighted] = feature_subtraction_weighted   \n",
    "     \n",
    "\n",
    "neighbours_relationship(landslides_seg,'ndvi','area_m2')             \n",
    "neighbours_relationship(landslides_seg,'ratio_rg_change','area_m2')\n",
    "neighbours_relationship(landslides_seg,'brightness','area_m2')\n",
    "neighbours_relationship(landslides_seg,'gndvi','area_m2')\n",
    "neighbours_relationship(landslides_seg,'ndvi_change','area_m2')\n",
    "neighbours_relationship(landslides_seg,'brightness_change','area_m2')\n",
    "neighbours_relationship(landslides_seg,'nd_std','area_m2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate relative relieve: difference between the highest and lowest points in elevation within the segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_relief (df_train,height_min, height_max):\n",
    "    \n",
    "     for key in df_train:\n",
    "            relative_relief_list = []  \n",
    "            \n",
    "            for i in range (len(df_train[key])):\n",
    "                relative_relief = (df_train[key][height_max].iloc[i] - df_train[key][height_min].iloc[i] )\n",
    "                relative_relief_list.append (relative_relief)\n",
    "                \n",
    "            df_train[key]['relative_relief'] = relative_relief_list  \n",
    "\n",
    "relative_relief (landslides_seg, 'height_min', 'height_max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a unique dataset with all segmented tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keys = pd.concat(landslides_seg, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of landslides segments\n",
    "df_keys.loc[df_keys.loc[:,'class']==1,:].count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of non- landslides segments\n",
    "df_keys.loc[df_keys.loc[:,'class']==0,:].count() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize data for visualization purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create a new df to normalize the features values\n",
    "df_norm_data = df_keys.copy()\n",
    "df_norm_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_normalization=[]\n",
    "# df_keys.columns\n",
    "feature_normalization.append((df_norm_data.columns[2], df_norm_data.columns[3], df_norm_data.columns[4],  \n",
    "                              df_norm_data.columns[5], df_norm_data.columns[6],df_norm_data.columns[7],\n",
    "                              df_norm_data.columns[10],df_norm_data.columns[11],df_norm_data.columns[12],\n",
    "                              df_norm_data.columns[13], df_norm_data.columns[14], df_norm_data.columns[15],  \n",
    "                              df_norm_data.columns[16], df_norm_data.columns[17],df_norm_data.columns[18],\n",
    "                              df_norm_data.columns[19],df_norm_data.columns[20],df_norm_data.columns[21],\n",
    "                              df_norm_data.columns[22],df_norm_data.columns[23], df_norm_data.columns[24],\n",
    "                              df_norm_data.columns[25], df_norm_data.columns[26]))\n",
    "                             \n",
    "feature_normalization= feature_normalization[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(feature,norm_data):\n",
    "    min_val = norm_data[feature].min() # record scaling minimum\n",
    "    max_val = norm_data[feature].max() # record scaling maximum\n",
    "    norm_data[feature] = (norm_data[feature] - min_val) / (max_val - min_val)\n",
    "for i in range (len(feature_normalization)):\n",
    "    normalization(feature_normalization[i],df_norm_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Data Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sns.set(font_scale=1.5)\n",
    "# sns_plot = sns.pairplot(df_norm_data ,hue='class', palette='deep', vars=['ndvi', 'brightness_change_deviation','ratio_rg_change_deviation','brightness','relative_relief'],height = 4) #\"b4\", \"b3\", \"b2\",\n",
    "# sns_plot.savefig(correlation_graph_output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as performance\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from plot_metric.functions import BinaryClassification\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from inspect import signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define X (observations) and Y (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_keys ['class'] \n",
    "\n",
    "X = df_keys[['ndvi','ratio_rg_change_deviation','brightness_change_deviation','ndvi_change_deviation','brightness','slope_mean',\n",
    "                'gndvi_deviation','slope_max','nd_std','relative_relief']]\n",
    "# ,,,\n",
    "\n",
    "# #creates arrays for X and y\n",
    "y_array= y.values\n",
    "X_array= X.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the name of the columns (to create the feature importance graph in step x)\n",
    "feature_list = X.columns\n",
    "feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset 70%(training) 30%(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset in training and testing\n",
    "def model_split(X,y):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=28) \n",
    "    return X_train,X_test,y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name of the classes: 1 = landslides; 0 =  non-landslides\n",
    "class_name = df_keys['class'].unique()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    "#    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           #label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='Actual class',\n",
    "           xlabel='Predicted class')\n",
    "#     plt.grid(b=None)\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    #export the confusion matrix\n",
    "    fig.savefig(confusion_matrix_output_path ) \n",
    "    \n",
    "    return ax \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(classifier,X_train):\n",
    "    \n",
    "     importances = list(classifier.feature_importances_)\n",
    "    \n",
    "    # List of tuples with variable and importance\n",
    "     feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "    # Sort the feature importances by most important first\n",
    "     feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_graph(classifier):\n",
    "    \n",
    "     # Get numerical feature importances\n",
    "     importances = list(classifier.feature_importances_)\n",
    "    \n",
    "    # List of tuples with variable and importance\n",
    "     feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "     # Sort the feature importances by most important first\n",
    "     feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "     # list of x locations for plotting\n",
    "     x_values = list(range(len(importances)))\n",
    "\n",
    "    # Make a bar chart\n",
    "     plt.bar(x_values, importances, orientation = 'vertical', linewidth = 0.9) #color = 'b', edgecolor = 'b'\n",
    "\n",
    "     # Tick labels for x axis\n",
    "     plt.xticks(x_values, feature_list, rotation='vertical')\n",
    "\n",
    "    # Axis labels and title\n",
    "     plt.ylabel('Importance'); plt.xlabel(''); plt.title('Feature Importances');    \n",
    "        \n",
    "     plt.savefig(feature_importance_output_path) \n",
    "    \n",
    "    # List of features sorted from most to least important\n",
    "     sorted_importances = [importance[1] for importance in feature_importances]\n",
    "     sorted_features = [importance[0] for importance in feature_importances]\n",
    "    \n",
    "    # #     # Print out the feature and importances \n",
    "     [print('Variable: {:20}Importance: {}'.format(*pair)) for pair in feature_importances];    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_performance(y_test,prediction_sampling):\n",
    "    \n",
    "        print( '\\n'+'Classification_report:'+'\\n'+'\\n',classification_report(y_test,prediction_sampling))\n",
    "        plot_confusion_matrix(y_test, prediction_sampling, classes=class_name,title='Confusion matrix')\n",
    "        \n",
    "        return classification_report(y_test,prediction_sampling, output_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rforest = RandomForestClassifier(n_estimators=50, max_depth=40,bootstrap=True, class_weight={0:1,1:5},random_state=82,criterion=\"gini\", min_samples_leaf=6, min_samples_split=4,max_features= 'auto')\n",
    "rforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model (model,X_train, y_train, X_test,y_test):\n",
    "    \n",
    "    model.fit (X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    metrics = model_performance(y_test,predictions)\n",
    "    print ('------------------------------------------------------'+ '\\n')\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_run(model, X,y): \n",
    "    \n",
    "    X_train,X_test,y_train, y_test = model_split(X,y)\n",
    "\n",
    "    train_model (model,X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    feature_importance(model,X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(rforest,X_array,y_array) #create the confusion matrix for the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_graph(rforest) #ranking the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Persistance\n",
    "\n",
    " Persist the model for future use without having to retrain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model = df_keys ['class'] \n",
    "\n",
    "X_model = df_keys[['ndvi','ratio_rg_change_deviation','brightness_change_deviation','ndvi_change_deviation','brightness','slope_mean',\n",
    "                'gndvi_deviation','slope_max','nd_std','relative_relief']]\n",
    "\n",
    "#B8  #relativ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_array_= y_model.values\n",
    "X_array_= X_model.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf_classifier =   RandomForestClassifier(n_estimators=50, max_depth=40,bootstrap=True, class_weight={0:1,1:5},random_state=82,criterion=\"gini\", min_samples_leaf=6, min_samples_split=4,max_features= 'auto')\n",
    "rforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier.fit (X_array_, y_array_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import mkdtemp\n",
    "savedir = mkdtemp()\n",
    "import os\n",
    "from joblib import dump, load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "filename =  model_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(rf_classifier, filename) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
